# import os
# import json
# import numpy as np
# from pathlib import Path
# from datetime import datetime

# def analyze_dataset_structure(dataset_path="./btc_2025_06_07"):
#     dataset_path = Path(dataset_path)

#     print("=" * 80)
#     print(f"BTCæ•°æ®é›†ç»“æ„åˆ†æ - {dataset_path}")
#     print("=" * 80)

#     if not dataset_path.exists():
#         print(f"âŒ é”™è¯¯: è·¯å¾„ {dataset_path} ä¸å­˜åœ¨")
#         return

#     structure_output = ""
#     print("\nğŸ“ ç›®å½•ç»“æ„:")
#     print("-" * 50)

#     total_dirs = 0
#     total_files = 0
#     file_types = {}
#     file_sizes = []

#     def print_tree(path, prefix="", is_last=True):
#         nonlocal total_dirs, total_files, structure_output

#         items = sorted(list(path.iterdir()))
#         dirs = [item for item in items if item.is_dir()]
#         files = [item for item in items if item.is_file()]

#         for i, dir_path in enumerate(dirs):
#             is_last_dir = (i == len(dirs) - 1) and len(files) == 0
#             connector = "â””â”€â”€ " if is_last_dir else "â”œâ”€â”€ "
#             line = f"{prefix}{connector}ğŸ“ {dir_path.name}/"
#             print(line)
#             structure_output += line + "\n"
#             total_dirs += 1

#             if len(prefix) < 40:
#                 extension = "    " if is_last_dir else "â”‚   "
#                 print_tree(dir_path, prefix + extension, is_last_dir)

#         for i, file_path in enumerate(files):
#             is_last_file = i == len(files) - 1
#             connector = "â””â”€â”€ " if is_last_file else "â”œâ”€â”€ "

#             file_size = file_path.stat().st_size
#             file_sizes.append(file_size)
#             total_files += 1

#             ext = file_path.suffix.lower()
#             file_types[ext] = file_types.get(ext, 0) + 1

#             size_str = format_file_size(file_size)
#             line = f"{prefix}{connector}ğŸ“„ {file_path.name} ({size_str})"
#             print(line)
#             structure_output += line + "\n"

#     print_tree(dataset_path)

#     print(f"\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
#     print("-" * 50)
#     print(f"æ€»ç›®å½•æ•°: {total_dirs}")
#     print(f"æ€»æ–‡ä»¶æ•°: {total_files}")

#     if file_sizes:
#         total_size = sum(file_sizes)
#         print(f"æ€»å¤§å°: {format_file_size(total_size)}")
#         print(f"å¹³å‡æ–‡ä»¶å¤§å°: {format_file_size(total_size / len(file_sizes))}")

#     print(f"\nğŸ“‹ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
#     for ext, count in sorted(file_types.items()):
#         ext_display = ext if ext else "æ— æ‰©å±•å"
#         print(f"  {ext_display}: {count} ä¸ªæ–‡ä»¶")

#     generate_dataset_report(dataset_path, total_dirs, total_files, file_types, file_sizes, structure_output)


# def format_file_size(size_bytes):
#     if size_bytes == 0:
#         return "0B"
#     size_names = ["B", "KB", "MB", "GB", "TB"]
#     i = int(np.floor(np.log(size_bytes) / np.log(1024)))
#     p = pow(1024, i)
#     s = round(size_bytes / p, 2)
#     return f"{s} {size_names[i]}"


# def generate_dataset_report(dataset_path, total_dirs, total_files, file_types, file_sizes, structure_output):
#     print(f"\nğŸ“‹ æ•°æ®é›†ç»“æ„æŠ¥å‘Šç”Ÿæˆä¸­...")
#     print("=" * 50)

#     current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#     txt_content = f"""BTCæ•°æ®é›†ç»“æ„åˆ†ææŠ¥å‘Š
# {"=" * 80}
# åˆ†ææ—¶é—´: {current_time}
# æ•°æ®é›†è·¯å¾„: {dataset_path}

# ç›®å½•ç»“æ„ç»Ÿè®¡:
# {"=" * 50}
# æ€»ç›®å½•æ•°: {total_dirs}
# æ€»æ–‡ä»¶æ•°: {total_files}
# æ€»å¤§å°: {format_file_size(sum(file_sizes)) if file_sizes else '0B'}
# å¹³å‡æ–‡ä»¶å¤§å°: {format_file_size(sum(file_sizes)/len(file_sizes)) if file_sizes else '0B'}

# æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:
# {"-" * 30}
# """
#     for ext, count in sorted(file_types.items()):
#         ext_display = ext if ext else "æ— æ‰©å±•å"
#         txt_content += f"{ext_display}: {count} ä¸ªæ–‡ä»¶\n"

#     txt_content += f"""
# è¯¦ç»†ç›®å½•ç»“æ„:
# {"=" * 50}
# {structure_output}
# """

#     # ä¿å­˜ TXT
#     txt_path = Path.cwd() / f"dataset_structure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
#     try:
#         with open(txt_path, 'w', encoding='utf-8') as f:
#             f.write(txt_content)
#         print("\nğŸ“ TXT æŠ¥å‘Šå·²ä¿å­˜è‡³ä»¥ä¸‹è·¯å¾„:")
#         print("=" * 60)
#         print(txt_path)
#         print("=" * 60)
#     except Exception as e:
#         print(f"âŒ ä¿å­˜ TXT æŠ¥å‘Šå¤±è´¥: {e}")

#     # ä¿å­˜ JSON
#     json_report = {
#         "dataset_path": str(dataset_path),
#         "analysis_time": datetime.now().isoformat(),
#         "summary": {
#             "total_directories": total_dirs,
#             "total_files": total_files,
#             "total_size": format_file_size(sum(file_sizes)) if file_sizes else '0B',
#             "average_file_size": format_file_size(sum(file_sizes)/len(file_sizes)) if file_sizes else '0B',
#             "file_types": file_types
#         },
#         "directory_structure": structure_output.splitlines()
#     }

#     json_path = Path.cwd() / f"dataset_structure_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
#     try:
#         with open(json_path, 'w', encoding='utf-8') as f:
#             json.dump(json_report, f, indent=2, ensure_ascii=False)
#         print("\nğŸ“ JSON æŠ¥å‘Šå·²ä¿å­˜è‡³ä»¥ä¸‹è·¯å¾„:")
#         print("=" * 60)
#         print(json_path)
#         print("=" * 60)
#     except Exception as e:
#         print(f"âŒ ä¿å­˜ JSON æŠ¥å‘Šå¤±è´¥: {e}")


# if __name__ == "__main__":
#     analyze_dataset_structure("./data/btc_2025_06_07")

#     print("\nğŸ“Œ ä½¿ç”¨æç¤ºï¼š")
#     print("# æœ¬å·¥å…·ä¸“æ³¨äºäº†è§£ç›®å½•ç»“æ„ã€æ•°æ®ç»„ç»‡æ–¹å¼å’Œæ–‡ä»¶åˆ†å¸ƒ")
#     print("# å¯ç”¨äºåˆæ­¥å®¡æŸ¥æ•°æ®é›†å†…å®¹ã€æ„å»ºç´¢å¼•ã€æ•°æ®æ¸…æ´—å‰æ£€æŸ¥ç­‰åœºæ™¯")


import pandas as pd

csv_path = "data/btc_2025_06_07/spot/trades/data/spot/monthly/trades/BTCUSDT/2025-06-01_2025-07-31/BTCUSDT-trades-2025-06.csv"

try:
    df = pd.read_csv(csv_path, nrows=10)
    print("âœ… æˆåŠŸè¯»å–å‰10è¡Œæ•°æ®ï¼š\n")
    print(df)
except Exception as e:
    print(f"âŒ è¯»å–å¤±è´¥: {e}")
